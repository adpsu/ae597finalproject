from pyspark.sql import SparkSession
from pyspark.sql.functions import col, substring, lpad
import geopandas as gpd
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
import numpy as np

# Initialize Spark session
spark = SparkSession.builder.appName("Heatmap Analysis").getOrCreate()

# Load the parquet files using PySpark
file_path = '/path/to/table.parquet'  # Replace with actual folder path
df_spark = spark.read.parquet(file_path)

# Filter by scenario and subsector conditions in PySpark
df_filtered_spark = df_spark.filter(
    (df_spark["scenario"] == "High+unmanaged") & 
    (df_spark["subsector"].startswith("Personal_"))
)

# Extract state from county and ensure state codes are two digits
df_filtered_spark = df_filtered_spark.withColumn("state", lpad(substring("county", 1, 2), 2, "0"))

# Sum 'value' for each time_est and county using PySpark groupBy
df_grouped_spark = df_filtered_spark.groupBy("time_est", "county", "state").sum("value")
df_grouped_spark = df_grouped_spark.withColumnRenamed("sum(value)", "value")

# Write the aggregated data to a temporary parquet file
temp_parquet_path = '/path/to/temp_output.parquet'  # Replace with a suitable path
df_grouped_spark.write.mode('overwrite').parquet(temp_parquet_path)

# Process data in chunks with GeoPandas
chunk_reader = pd.read_parquet(temp_parquet_path, engine='pyarrow', chunksize=100000)  # Adjust chunk size as needed
us_counties = gpd.read_file('tl_2019_us_county.shp')  # Replace with actual shapefile path
us_counties['state'] = us_counties['GEOID'].str[:2]
us_counties = us_counties[['GEOID', 'geometry']]
us_counties = us_counties.rename(columns={'GEOID': 'county'})

# Loop through chunks for plotting
for chunk in chunk_reader:
    # Merge with GeoPandas shapefile
    chunk['county'] = chunk['county'].astype(str)  # Ensure type consistency
    merged = us_counties.merge(chunk, on='county', how='left')
    
    # Plot heatmap for the chunk
    def plot_heatmap(data, title):
        avg_value = data['value'].mean()
        data['percent_deviation'] = (data['value'] - avg_value) / avg_value * 100

        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        data.plot(column='percent_deviation', cmap='coolwarm', legend=True, ax=ax, norm=Normalize(vmin=-100, vmax=100))
        ax.set_title(title)
        plt.axis('off')
        plt.show()
    
    plot_heatmap(merged, "Chunk Heatmap | County Level")
